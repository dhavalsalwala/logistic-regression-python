{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZeE6768jYg1g"
   },
   "source": [
    "# Machine Learning Algorithm: Logistic Regression\n",
    "Logistic regression is a predictive analysis classification technique to conduct when the dependent variable is binary. Although it can be exetended for multi-class target variable via one-vs-all strategy. It is used to describe data and to explain the relationship between one dependent binary variable and one or more features or independent variables. \n",
    "\n",
    "## Assumptions\n",
    "  * The dependent variable should be binary in nature viz. true / false.\n",
    "  * There should be no outliers/anomaly in source data presented.\n",
    "  * There should be no high correlations among the input variables. \n",
    "  * Logistic regression requires a large sample sizes.\n",
    "  \n",
    "The goal of a Logistic Regression is to construct a model: a hypothesis that can be used to estimate Y based on X. We used Linear regression to build our classification model. Given target value (y) and set of attribute values (x), the goal of Linear Regression is to find an equation that describes the target in terms of the attributes.\n",
    " y = θ0 + θ1x1 + θ2x2 + … + θmxm, where θm is the weight associated with attribute xm. \n",
    " \n",
    "Hypothesis is given by the equation:hθ(x) = θ0x0 + θ1x1...θnxn, where our goal is to choose θ0, θ1...θn such that for each training example (x(i), y(i)), hθ(x(i)) is as close as possible to y(i) on average. We used squared error cost function J(θ0, θ1...θn) to minimize J to find the optimal hypothesis. There are two main ways to find θ0, θ1, ...θn - <b>Closed Form Solution and Batch Gradient Descent.</b>\n",
    "\n",
    "## Batch Gradient Descent: \n",
    "The gradient descent algorithm starts with an initial set of values and iteratively moves toward a set of parameter values that minimize the function. In linear regression, we need to fit our data better to the given hypothesis which is defined by the Error function. If we minimize this function, we will get the best line for our data. To compute it, we will need to differentiate the error function. In our case, we have multiple input variables and class variables. Hence, we will need to compute a partial derivative for each of them.\n",
    "\n",
    "<img src=\"data/update_weights.png\" style=\"width: 300px;float:left\"/>\n",
    "\n",
    "Where, \n",
    "* J is a convex quadratic function. \n",
    "* The intuition behind the convergence is that J(θ) approaches 0 as we approach the bottom of our convex function.  Regardless of where we started, gradient descent will eventually arrive at the global minimum\n",
    "\n",
    "  ### Features\n",
    "  * General-purpose algorithm for finding a local minimum of any continuous differentiable function\n",
    "  * Iterative; not as efficient as closed form\n",
    "  * Applicable to many optimisation tasks\n",
    "  \n",
    "## Worked Example\n",
    "\n",
    "We now move forward to our owl classification example. There are three types: Barn Owl, Snowy Owl & Long-Eared Owl. Our job is to build a classifier which distinguishes between them given several features. We have been given owls.csv file. Each line in the file describes one type of Owl found in Ireland: body-length, wing-length, body-width, wing-width, type. As this is a multi-class classification problem, we will be using one-vs-all strategy for computing the results.\n",
    "\n",
    "## Note: \n",
    "All the Util functions used in below classifier are kept in /model/utils.py and /common/utils.py\n",
    "\n",
    "### Interactive UI: Classification_run.py\n",
    "\n",
    "When u run the the program via <B>classification_run.py</B>, you will be prompted to enter filename, column names and the target attribute.\n",
    "\n",
    "<img src=\"console.png\" style=\"width: 1000px;float:left\"/>\n",
    "\n",
    "### Pre-processing: SupervisedLearningTester.py\n",
    "\n",
    "As a part of pre-processing, it import source file and perform various operations using python pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s-e0Ptu8tfj7"
   },
   "outputs": [],
   "source": [
    "def test_logistic_regression(file_path, column_names, target_column):\n",
    "    print()\n",
    "    print(Color.BOLD+Color.PURPLE + \"==>Pre-processing source file:\"+file_path+Color.END)\n",
    "\n",
    "    df = pd.read_csv(file_path, header=0)\n",
    "    \n",
    "    #Assingning names to the columns as per the data set\n",
    "    df.columns=column_names.split(\",\")\n",
    "    #Re-numbering data set index from 1 (using numpy)\n",
    "    df.index = np.arange(1, len(df) + 1)\n",
    "\n",
    "    #Mapping data set into dependent attributes and class label attribute\n",
    "    X = df.drop(target_column,axis=1)\n",
    "    y = np.array(df[target_column])\n",
    "    \n",
    "    #Converting target class attributes into numerical ones.\n",
    "    y = TestUtil.replace_target_class_values(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4QMVqnsuw4sI"
   },
   "source": [
    "Now we are ready to feed our data to our hypothesis. Let's buld Logistic Regression model step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PwAeaTkZtd6s"
   },
   "source": [
    "\n",
    "\n",
    "### Hypothesis Representation\n",
    "* In order to perform classification using Logistic Regression we need to modify our hypothesis so that it gives values between 0 and 1. we used sigmoid function (also known as Logistic function).\n",
    "\n",
    "<img src=\"sigmoid.png\" style=\"width: 200px;float:left\"/><br><br><br><br><br><br>\n",
    "\n",
    "<br><br>\n",
    "When z approaches positive infinity, g(Z) will tend to 1. Similarly, when z approaches negative infinity, g(z) will tend to 0.\n",
    "### Sigmoid function Implementation:\n",
    "* Takes the z=g(x) valie and applies logisic function.\n",
    "* Output is always between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-cAQTNsHpI7p"
   },
   "outputs": [],
   "source": [
    "def _logistic_function(self, z):\n",
    "        i = 1 / (1 + np.exp(-z))\n",
    "        return i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis function\n",
    "* Takes input attributes (sparse matrix form) and the corresponding theta values.\n",
    "* Invoke logistic function for each of the input attribute set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _hypothesis(self, X,theta_vector):\n",
    "        z = 0.0\n",
    "        for i in range(len(theta_vector)):\n",
    "            z += X.item(i)*theta_vector.item(i)\n",
    "        return self._logistic_function(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JEVqa--xqgWE"
   },
   "source": [
    "### Cost Function\n",
    "We cannot use the same cost function of linear regression because the Logistic Function will cause the output to be wavy, causing many local optima. In other words, it will not be a convex function.\n",
    "\n",
    "Instead, our cost function for logistic regression looks like:\n",
    "<br><img src=\"update_weights.png\" style=\"width: 450px;float:left\" />\n",
    "<br><br><br><br><br><br><br><br><br>\n",
    "If we take partial derivative of this cost function using calculus and apply it to gradient descent: <br><img src=\"gradient.png\" style=\"width: 450px;float:left\" />\n",
    "<br><br><br><br><br><br><br>\n",
    "\n",
    "### Batch Gradient Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oxZsUTjSqk2s"
   },
   "outputs": [],
   "source": [
    "def _batch_gradient_descent(self, X,y,j,theta_vector):\n",
    "        sum = 0\n",
    "        for i in range(len(y)):\n",
    "            sum+=(self._hypothesis(X[i],theta_vector)-y[i])*X[i][j]\n",
    "        return sum/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier - linear_model.py: LogisticRegression()\n",
    "It has four default parameters:\n",
    "* alpha (learning rate): 0.009\n",
    "* tol (tolerance level): 10^(-3)\n",
    "* maxiter (maximum iteration): 2000\n",
    "* multi_class (multi class notifier): auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, alpha=0.009, tol=1e-3, maxiter=2000, multi_class=\"auto\"):\n",
    "        self.alpha = alpha\n",
    "        self.tol = tol\n",
    "        self.maxiter = maxiter\n",
    "        self.multi_class = multi_class\n",
    "        self.nd_theta_vector = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit method\n",
    "\n",
    "* The fit method accepts input attribute(sparse matrix) and target attribute(array-like structure)\n",
    "* It converts input and target attributes into the array-like structure.\n",
    "* Then it adds intercept (X0=1) to the input matrix.\n",
    "* Finally on the basis of multi_class variable it calls the respective fit method.\n",
    "\n",
    "    ### fit_binary_model\n",
    "    * Initialize theta vector to zero and delta vector to one(to avoid tolerance check initially)\n",
    "    * Then it calculates cos function using _batch_gradient_descent in a loop for @maxiter times.\n",
    "    * Finally when the function converges, the theta vector is added to the list.\n",
    "    \n",
    "  ### fit_multinomial_model\n",
    "  * It is called in case of the multi-class classification problem.\n",
    "  * It maps the target variable into a binary classification problem (assigning 1 and 0 accordingly)  via one-vs-all strategy.\n",
    "  * It calls fit_binary_model iteratively until all combination of multi-class is exhausted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def fit(self, X, y):\n",
    "        self.nd_theta_vector = None\n",
    "        X = ArrayUtil.convert_to_array(X)\n",
    "        y = ArrayUtil.convert_to_array(y)\n",
    "        X = LinearUtil.append_intercept(X)\n",
    "        self.multi_class = LinearUtil.check_target_class_type(self.multi_class, y)\n",
    "\n",
    "        if self.multi_class == \"binary\":\n",
    "            self._fit_binary_model(X, y)\n",
    "        elif self.multi_class == \"multinomial\":\n",
    "            self._fit_multinomial_model(X, y)\n",
    "        else:\n",
    "            raise InvalidInputException(Color.BOLD+Color.RED+'Invalid attribute value:multi_class='+self.multi_class+'. Please enter value as {auto, binary or multinomial}'+Color.END)\n",
    "\n",
    "    def _fit_multinomial_model(self, X, y):\n",
    "        self.target_class_vector = set(y)\n",
    "        for i in self.target_class_vector:\n",
    "            modified_y = np.array([1 if o == i else 0 for o in y])\n",
    "            self._fit_binary_model(X, modified_y)\n",
    "        self.plot_graph(X,y)\n",
    "\n",
    "    def _fit_binary_model(self, X, y):\n",
    "\n",
    "        theta_vector = ArrayUtil.create_zero_vector(X)\n",
    "        delta_vector = ArrayUtil.create_one_vector(X)\n",
    "        itr = 0\n",
    "        while LinearUtil.check_tolerance(delta_vector,self.tol) and itr < self.maxiter:\n",
    "\n",
    "            for j in range(len(theta_vector)):\n",
    "                theta = theta_vector[j] - (self.alpha * self._batch_gradient_descent(X, y, j, theta_vector))\n",
    "                delta_vector[j] = theta\n",
    "\n",
    "            theta_vector = delta_vector\n",
    "            itr = itr + 1\n",
    "\n",
    "        if self.nd_theta_vector is None:\n",
    "            self.nd_theta_vector = theta_vector\n",
    "        else:\n",
    "            self.nd_theta_vector = np.vstack([self.nd_theta_vector, theta_vector])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict\n",
    "* It is called when the multi_class variable is binary.\n",
    "* it pre-processes the test data by converting it to an array like structure and adding intercept.\n",
    "* It predicts target attribute by applying obtained theta values to the test data.\n",
    "* Process continues until all the test sets are completed.\n",
    "* Finally, predicted values are mapped via threshold (y=1 if predicted>=0.5 or y=0 if predicted<0)\n",
    "\n",
    "### predict_multinomial\n",
    "* It is called when the multi_class variable is multinomial.\n",
    "* it pre-processes the test data by converting it to an array like structure and adding intercept.\n",
    "* Predicted values from all the different hypothesis are stored in a temporary list.\n",
    "* It finds the maximum probability of all the values from the temporary list and assigns it to resultant vector.\n",
    "* Process continues in a loop until all the test sets are completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def predict(self, X, threshold=0.5):\n",
    "\n",
    "        if self.multi_class == \"multinomial\":\n",
    "            return self.predict_multinomial(X)\n",
    "\n",
    "        X = ArrayUtil.convert_to_array(X)\n",
    "        X = LinearUtil.append_intercept(X)\n",
    "        predicted_values=[]\n",
    "        for i in range(X.shape[0]):\n",
    "            predicted_values.append(self._hypothesis(X[i], self.nd_theta_vector))\n",
    "        return np.array([1 if o>=threshold else 0 for o in predicted_values])\n",
    "\n",
    "    def predict_multinomial(self, X):\n",
    "\n",
    "        X = ArrayUtil.convert_to_array(X)\n",
    "        X = LinearUtil.append_intercept(X)\n",
    "\n",
    "        result_array = np.zeros(X.shape[0])\n",
    "        for i in range(X.shape[0]):\n",
    "            temp_predict_list = []\n",
    "            for theta in self.nd_theta_vector:\n",
    "                temp_predict_list.append(self._hypothesis(X[i], theta))\n",
    "            result_array[i] = list(self.target_class_vector)[np.argmax(temp_predict_list)]\n",
    "        return np.array(result_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Program: SupervisedLearningTester.py\n",
    "* Create a model of the type LogisticRegression()\n",
    "* Iteratively split the file into 2/3 for training, 1/3 for testing with shuffling data every time. \n",
    "* Calling predict function on 1/3 of the test data. \n",
    "* Predicted values are then used to compute accuracy and confusion matrix for each of the iterations.\n",
    "* Finally, computing average accuracy of all the 10 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-2-98072dcdb8fc>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-98072dcdb8fc>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    lr_accuracy_score = []  # list to store accuracy for all 10 combination of data sets.\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    " model = LogisticRegression()\n",
    "    lr_accuracy_score = []  # list to store accuracy for all 10 combination of data sets.\n",
    "    for itr in range(10):\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.33, Shuffle=True)\n",
    "        model.fit(X_train, Y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy=LinearUtil.calculate_accuracy(Y_test, y_pred)\n",
    "        print(Color.BOLD+Color.PURPLE + \"Fold \"+str(itr+1)+\" Accuracy: \"+str(accuracy)+Color.END)\n",
    "        lr_accuracy_score.append(accuracy)\n",
    "        LinearUtil.confusion_matrix(Y_test, y_pred)\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(Color.BOLD+Color.PURPLE + \"Average accuracy of a model after 10 fold is \" + str(sum(lr_accuracy_score) / len(lr_accuracy_score))+Color.END)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations and Test Results: (for entire logs, please see Appendix console.log)\n",
    "* Results as taken from PyCharm_2018.1.4(Python IDE) console window.\n",
    "* Entire console log is attached with this report.\n",
    "* It displays theta values for 3 hypothesis (as there were 3 target type attributes) found by the model.\n",
    "\n",
    "<img src=\"observations.png\" style=\"width: 600px;float:left\"/>\n",
    "<br><br><br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "### These are the final 3 weights (3 class problem):\n",
    "\n",
    "Finished after  2000  iterations: theta= [ 0.14526234 -0.6985769   0.00495833  0.39798835 -0.23206944]\n",
    "\n",
    "Finished after  2000  iterations: theta= [ 0.21249024  1.14925333  0.34541407 -1.83277407 -0.80193038]\n",
    "\n",
    "Finished after  2000  iterations: theta= [-0.45363001 -1.02217097 -0.91944209  1.4997036   1.05582103]\n",
    "\n",
    "### Confusion Matrix \n",
    "We can learn from the confusion matrix as follows:\n",
    " * The total class 0 in the dataset is the sum of the values on the 0 column (14)\n",
    " * The total class 1 in the dataset is the sum of the values on the 1 column (13)\n",
    " * The total class 2 in the dataset is the sum of the values on the 2 column (15)\n",
    " * The correct values are organized in a diagonal line from top left to bottom-right of the matrix (14+13+15).\n",
    " * More errors were made by predicting 2 as 0 than predicting other (0,1) classes.\n",
    "\n",
    "<img src=\"confusion_matrix.png\" style=\"width: 300px;float:left\"/>\n",
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "### Summary: Prediction Accuracy\n",
    "* Avearge: 0.8800\n",
    "* Standard Deviation: 0.0669\n",
    "* Median: 0.8999\n",
    "\n",
    "* only for Iteration 1: Macro average\n",
    "    * Precison: 0.94444444444444453\n",
    "    * Recall: 0.94117647058823517\n",
    "    * F_score: 0.93743890518084072\n",
    "\n",
    "<b>Picking a learning rate = 0.009 and number of iterations = 2000 the algorithm classified all instances successfully with the accuracy rate of 0.88. Gradient descent only minimizes the cost if the learning rate is low enough. So if we trained our model with a smaller learning rate and more iterations we would find approximately equal weights.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* Scikit-learn packages and its dependencies. http://scikit-learn.org Accessed 1 Dec, 2018 \n",
    "* Matplotlib library. https://matplotlib.org/contents.html Accessed 1 Dec, 2018\n",
    "* Pandas Library. http://pandas.pydata.org/pandas-docs/stable/ Accessed 1 Dec, 2018\n",
    "* Coursera Machine Learning course: https://www.coursera.org/learn/machine-learning/home/welcome Accessed 1 Dec, 2018\n",
    "\n",
    "<br><br><br><br><br><br><br><br><br><br>\n",
    "## Appendix\n",
    "### classification_run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Starting point for classification program.\n",
    "Run this file to execute Logistic Regression.\n",
    "'''\n",
    "from tester.SupervisedLearningTester import test_logistic_regression\n",
    "from common.utils import Color\n",
    "\n",
    "print (Color.BOLD+Color.PURPLE + \"-----------------------------------------------------------------------------------------------------------------------------\"+Color.END)\n",
    "print (Color.BOLD+Color.PURPLE + \"Demonstration of supervised classification based Machine Learning using Logistic Regression. v1.0 - Dhaval Salwala (18230845)\"+Color.END)\n",
    "print (Color.BOLD+Color.PURPLE + \"Part of Continuous Assessment - Machine Learning and Data Mining [CT475], National University Of Ireland Galway\"+Color.END)\n",
    "print (Color.BOLD+Color.PURPLE + \"Tutor: Professor Michael Madden\"+Color.END)\n",
    "print (Color.BOLD+Color.PURPLE + \"-----------------------------------------------------------------------------------------------------------------------------\"+Color.END)\n",
    "print ()\n",
    "print (Color.BOLD+Color.RED+\"==> Please enter the absolute path of the classification data file. You can use one of the sample file present\"\n",
    "                 \" in the data directory of the project. \\nMake sure all the attributes are numeric except for the target attribute.\"+Color.END)\n",
    "print()\n",
    "file_path = input(Color.BOLD+\"1. Enter Absolute File Path (Valid Extension: .xls .xlsx .txt .csv) :\"+Color.END)\n",
    "column_names = input(Color.BOLD+\"2. Enter Column names (including target column) in the same order as in file seperated by comma without any space :\"+Color.END)\n",
    "target_column = input(Color.BOLD+\"3. Enter Target column name :\"+Color.END)\n",
    "\n",
    "print ()\n",
    "print (Color.BOLD+Color.PURPLE + \"==>Initialising Supervised Learning... Preparing for launch...\"+Color.END)\n",
    "test_logistic_regression(file_path,column_names,target_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tester/SupervisedLearningTester.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Module to test your classification program.\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from common.utils import Color\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from common.utils import TestUtil\n",
    "from model.utils import LinearUtil\n",
    "from model.classification.linear_model import Logistic_Regression\n",
    "\n",
    "\n",
    "def test_logistic_regression(file_path, column_names, target_column):\n",
    "    print()\n",
    "    print(Color.BOLD+Color.PURPLE + \"==>Pre-processing source file:\"+file_path+Color.END)\n",
    "\n",
    "    df = pd.read_csv(file_path, header=0) # reading from file\n",
    "    df.columns=column_names.split(\",\") # Assigning names to the columns as per the data set\n",
    "    df.index = np.arange(1, len(df) + 1) # Re-numbering data set index from 1 (using numpy)\n",
    "\n",
    "    # Mapping data set into dependent attributes and class label attribute\n",
    "    X = df.drop(target_column,axis=1)\n",
    "    y = np.array(df[target_column])\n",
    "    # Converting target class attributes into numerical ones.\n",
    "    y = TestUtil.replace_target_class_values(y)\n",
    "\n",
    "\n",
    "    print(Color.BOLD+Color.PURPLE + \"Applying 10 Fold Cross Validation.\"+Color.END)\n",
    "    # try block to catch any error\n",
    "    try:\n",
    "        model = Logistic_Regression()\n",
    "        lr_accuracy_score = []  # list to store accuracy for all 10 combination of data sets.\n",
    "        for itr in range(10):\n",
    "            X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.33, shuffle=True) # splits data into train/test\n",
    "            model.fit(X_train, Y_train) # fit model to train data\n",
    "            y_pred = model.predict(X_test) # predict values on train model\n",
    "            accuracy=LinearUtil.calculate_accuracy(Y_test, y_pred) #calculate accuracy\n",
    "            print(Color.BOLD+Color.PURPLE + \"Fold \"+str(itr+1)+\" Accuracy: \"+str(accuracy)+Color.END)\n",
    "            lr_accuracy_score.append(accuracy)\n",
    "            print (\"Precision, Recall and F_score: \"+str(precision_recall_fscore_support(Y_test, y_pred, average='macro')))\n",
    "            LinearUtil.confusion_matrix(Y_test, y_pred) # preparing confusion matrix\n",
    "    except Exception as e:\n",
    "        print(\"ERROR: \" + str(e))\n",
    "\n",
    "    print()\n",
    "    #computing average accuracy of all the 10 iterations.\n",
    "    print(Color.BOLD+Color.PURPLE + \"Average accuracy of a model after 10 fold is \" + str(sum(lr_accuracy_score) / len(lr_accuracy_score))+Color.END)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model/classification/linear_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Linear model class containing Linear_Regression\n",
    "Implementation of linear algorithms\n",
    "'''\n",
    "import numpy as np\n",
    "from common.utils import ArrayUtil\n",
    "from model.utils import LinearUtil\n",
    "from common.utils import Color\n",
    "from model.exception.linear_exception import InvalidInputException\n",
    "\n",
    "class Logistic_Regression:\n",
    "    def __init__(self, alpha=0.009, tol=1e-3, maxiter=2000, multi_class=\"auto\"):\n",
    "        # Initialising all default values\n",
    "        self.alpha = alpha\n",
    "        self.tol = tol\n",
    "        self.maxiter = maxiter\n",
    "        self.multi_class = multi_class\n",
    "        self.nd_theta_vector = None\n",
    "\n",
    "    # fit model to train data\n",
    "    def fit(self, X, y):\n",
    "        self.nd_theta_vector = None\n",
    "        X = ArrayUtil.convert_to_array(X) # converting train data to array-like\n",
    "        y = ArrayUtil.convert_to_array(y) # converting train data to array-like\n",
    "        X = LinearUtil.append_intercept(X) # adding intercept\n",
    "        self.multi_class = LinearUtil.check_target_class_type(self.multi_class, y)\n",
    "\n",
    "        if self.multi_class == \"binary\":\n",
    "            self._fit_binary_model(X, y)\n",
    "        elif self.multi_class == \"multinomial\":\n",
    "            self._fit_multinomial_model(X, y)\n",
    "        else:\n",
    "            raise InvalidInputException(Color.BOLD+Color.RED+'Invalid attribute value:multi_class='+self.multi_class+'. Please enter value as {auto, binary or multinomial}'+Color.END)\n",
    "\n",
    "    # fit multinomial model\n",
    "    def _fit_multinomial_model(self, X, y):\n",
    "        self.target_class_vector = set(y)\n",
    "        for i in self.target_class_vector:\n",
    "            modified_y = np.array([1 if o == i else 0 for o in y])\n",
    "            self._fit_binary_model(X, modified_y)\n",
    "\n",
    "    # fit binary model\n",
    "    def _fit_binary_model(self, X, y):\n",
    "\n",
    "        theta_vector = ArrayUtil.create_zero_vector(X)\n",
    "        delta_vector = ArrayUtil.create_one_vector(X)\n",
    "        itr = 0\n",
    "        while LinearUtil.check_tolerance(delta_vector,self.tol) and itr < self.maxiter:\n",
    "\n",
    "            # calling _batch_gradient_descent iteratively  until all the theta values have been evaluated.\n",
    "            for j in range(len(theta_vector)):\n",
    "                theta = theta_vector[j] - (self.alpha * self._batch_gradient_descent(X, y, j, theta_vector))\n",
    "                delta_vector[j] = theta\n",
    "\n",
    "            # assigning delta back to theta vector\n",
    "            theta_vector = delta_vector\n",
    "            itr = itr + 1\n",
    "\n",
    "        # storing theta values for each of trained hypothesis.\n",
    "        if self.nd_theta_vector is None:\n",
    "            self.nd_theta_vector = theta_vector\n",
    "        else:\n",
    "            self.nd_theta_vector = np.vstack([self.nd_theta_vector, theta_vector])\n",
    "\n",
    "        print(\"\\nFinished after \", itr, \" iterations: theta=\", theta_vector)\n",
    "\n",
    "    def _batch_gradient_descent(self, X, y, j, theta_vector):\n",
    "        sum = 0\n",
    "        for i in range(len(y)):\n",
    "            sum += (self._hypothesis(X[i], theta_vector) - y[i]) * X[i][j]\n",
    "        return sum / len(y)\n",
    "\n",
    "    # logistic regression hypothesis\n",
    "    def _hypothesis(self, X, theta_vector):\n",
    "        z = 0.0\n",
    "        for i in range(len(theta_vector)):\n",
    "            z += X.item(i) * theta_vector.item(i)\n",
    "        return self._logistic_function(z)\n",
    "\n",
    "    # sigmoid function\n",
    "    def _logistic_function(self, z):\n",
    "        i = 1 / (1 + np.exp(-z))\n",
    "        return i\n",
    "\n",
    "    # predict values for binary model\n",
    "    def predict(self, X, threshold=0.5):\n",
    "\n",
    "        if self.multi_class == \"multinomial\":\n",
    "            return self.predict_multinomial(X)\n",
    "\n",
    "        X = ArrayUtil.convert_to_array(X)\n",
    "        X = LinearUtil.append_intercept(X)\n",
    "        predicted_values=[]\n",
    "        for i in range(X.shape[0]):\n",
    "            predicted_values.append(self._hypothesis(X[i], self.nd_theta_vector))\n",
    "        return np.array([1 if o>=threshold else 0 for o in predicted_values]) # applying threshold before returning array.\n",
    "\n",
    "    # predict values for multinomial model\n",
    "    def predict_multinomial(self, X):\n",
    "\n",
    "        X = ArrayUtil.convert_to_array(X)\n",
    "        X = LinearUtil.append_intercept(X)\n",
    "\n",
    "        result_array = np.zeros(X.shape[0])\n",
    "        for i in range(X.shape[0]):\n",
    "            temp_predict_list = []\n",
    "            for theta in self.nd_theta_vector:\n",
    "                temp_predict_list.append(self._hypothesis(X[i], theta)) # iteratively call hypothesis for each of theta values.\n",
    "            result_array[i] = list(self.target_class_vector)[np.argmax(temp_predict_list)] # finding max of all hypothesis prediction\n",
    "        return np.array(result_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Utility Class for linear models\n",
    "'''\n",
    "import numpy as np\n",
    "from common.utils import Color\n",
    "from model.exception.linear_exception import InvalidInputException\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LinearUtil:\n",
    "\n",
    "    # check tolerance level of theta values\n",
    "    @staticmethod\n",
    "    def check_tolerance(delta_vector,tol):\n",
    "        stop_loop = False\n",
    "        for x in np.nditer(delta_vector):\n",
    "            if abs(x) > tol:\n",
    "                stop_loop = True\n",
    "                break\n",
    "        return stop_loop\n",
    "\n",
    "    # add intercept to input attributes\n",
    "    @staticmethod\n",
    "    def append_intercept(X):\n",
    "        x0 = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((x0, X), axis=1)\n",
    "\n",
    "    # check target type class value\n",
    "    @staticmethod\n",
    "    def check_target_class_type(multi_class, y):\n",
    "        if multi_class==\"auto\":\n",
    "            if len(set(y)) > 2:\n",
    "                return \"multinomial\"\n",
    "            else:\n",
    "                return \"binary\"\n",
    "        else:\n",
    "            return multi_class\n",
    "\n",
    "    # calculate accuracy of predicted results\n",
    "    @staticmethod\n",
    "    def calculate_accuracy(Y_test, y_pred):\n",
    "\n",
    "        if len(Y_test)!=len(y_pred):\n",
    "            raise InvalidInputException(\n",
    "                Color.BOLD + Color.RED + 'Both input should be of same length.' + Color.END)\n",
    "        else:\n",
    "            return (Y_test == y_pred).sum() / float(len(Y_test))\n",
    "\n",
    "    # create confusion matrix on predicted values\n",
    "    @staticmethod\n",
    "    def confusion_matrix(Y_test, y_pred):\n",
    "        y_actu = pd.Series(Y_test, name='Actual')\n",
    "        y_pred = pd.Series(y_pred, name='Predicted')\n",
    "        df_confusion = pd.crosstab(y_actu, y_pred)\n",
    "        print (\"Confusion Matrix: \"+str(df_confusion))\n",
    "        plt.matshow(df_confusion, cmap=plt.cm.gray_r)\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(len(df_confusion.columns))\n",
    "        plt.xticks(tick_marks, df_confusion.columns, rotation=45)\n",
    "        plt.yticks(tick_marks, df_confusion.index)\n",
    "        plt.ylabel(df_confusion.index.name)\n",
    "        plt.xlabel(df_confusion.columns.name)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### common/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Common utils to be used all by services\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ArrayUtil:\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_array(X):\n",
    "        if type(X) is not np.ndarray:\n",
    "            return np.array(X)\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    @staticmethod\n",
    "    def create_zero_vector(X):\n",
    "        return np.zeros(X.shape[1])\n",
    "\n",
    "    @staticmethod\n",
    "    def create_one_vector(X):\n",
    "        return np.ones(X.shape[1])\n",
    "\n",
    "\n",
    "class TestUtil:\n",
    "\n",
    "    # convert target attribute into numerical values\n",
    "    @staticmethod\n",
    "    def convert_target_column(values):\n",
    "        class_map = {}\n",
    "        i = 0\n",
    "        for val in values:\n",
    "            class_map[val] = i\n",
    "            i = i + 1\n",
    "        print(\"Converting target variable into numerical form with the following mapping.\")\n",
    "        print (class_map)\n",
    "        print()\n",
    "        return class_map\n",
    "\n",
    "    # Map target attribute value to its corresponding numerical value.\n",
    "    @staticmethod\n",
    "    def replace_target_class_values(y):\n",
    "        class_map = TestUtil.convert_target_column(set(y))\n",
    "        y = list(y)\n",
    "        for i in range(len(y)):\n",
    "            y[i] = class_map[y[i]]\n",
    "        return np.array(y)\n",
    "\n",
    "class Color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model/exception/linear_exception.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Exception class for Linear models\n",
    "'''\n",
    "class InvalidInputException(Exception):\n",
    "\n",
    "    def __init__(self, value):\n",
    "        self.parameter = value\n",
    "\n",
    "    def __str__(self):\n",
    "        return repr(self.parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Console Logs\n",
    "Demonstration of supervised classification based Machine Learning using Logistic Regression. v1.0 - Dhaval Salwala (18230845)\n",
    "Part of Continuous Assessment - Machine Learning and Data Mining [CT475], National University Of Ireland Galway\n",
    "#### Tutor: Professor Michael Madden\n",
    "-----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "==> Please enter the absolute path of the classification data file. You can use one of the sample file present in the data directory of the project. \n",
    "Make sure all the attributes are numeric except for the target attribute.\n",
    "\n",
    "1. Enter Absolute File Path (Valid Extension: .xls .xlsx .txt .csv) :/home/dsalwala/owls.csv\n",
    "2. Enter Column names (including target column) in the same order as in file seperated by comma without any space :body-length,wing-length,body-width,wing-width,type\n",
    "3. Enter Target column name :type\n",
    "\n",
    "==>Initialising Supervised Learning... Preparing for launch...\n",
    "\n",
    "==>Pre-processing source file:/home/dsalwala/owls.csv\n",
    "Converting target variable into numerical form with the following mapping.\n",
    "{'BarnOwl': 0, 'LongEaredOwl': 1, 'SnowyOwl': 2}\n",
    "\n",
    "Applying 10 Fold Cross Validation.\n",
    "\n",
    "Finished after  2000  iterations: theta= [ 0.02715753 -0.6842089   0.00638416  0.42579352 -0.35747045]\n",
    "\n",
    "Finished after  2000  iterations: theta= [ 0.22666919  1.1001088   0.37835726 -1.8160435  -0.79307123]\n",
    "\n",
    "Finished after  2000  iterations: theta= [-0.38342875 -1.00782828 -0.91257651  1.47442589  1.11500378]\n",
    "Fold 1 Accuracy: 0.933333333333\n",
    "Precision, Recall and F_score: (0.94444444444444453, 0.94117647058823517, 0.93743890518084072, None)\n",
    "Confusion Matrix: Predicted  0.0  1.0  2.0\n",
    "Actual                  \n",
    "0           14    0    3\n",
    "1            0   13    0\n",
    "2            0    0   15\n",
    "\n",
    "Finished after  2000  iterations: theta= [ 0.16848991 -0.66287538  0.08391121  0.26672085 -0.25977649]\n",
    "\n",
    "Finished after  2000  iterations: theta= [ 0.18746899  1.13338041  0.28596034 -1.76368468 -0.79024594]\n",
    "\n",
    "Finished after  2000  iterations: theta= [-0.4509548  -1.04976882 -0.91335917  1.52175478  1.06690595]\n",
    "Fold 2 Accuracy: 0.955555555556\n",
    "Precision, Recall and F_score: (0.95833333333333337, 0.94444444444444453, 0.94747474747474758, None)\n",
    "Confusion Matrix: Predicted  0.0  1.0  2.0\n",
    "Actual                  \n",
    "0           10    0    2\n",
    "1            0   19    0\n",
    "2            0    0   14\n",
    "\n",
    "Finished after  2000  iterations: theta= [ 0.11150799 -0.78957977  0.10219165  0.40857251 -0.27411633]\n",
    "\n",
    "Finished after  2000  iterations: theta= [ 0.22087141  1.14275363  0.32700018 -1.82601715 -0.79805189]\n",
    "\n",
    "Finished after  2000  iterations: theta= [-0.44589537 -0.95944881 -0.95229227  1.46196264  1.05619498]\n",
    "Fold 3 Accuracy: 0.977777777778\n",
    "Precision, Recall and F_score: (0.96296296296296291, 0.98333333333333339, 0.97184514831573654, None)\n",
    "Confusion Matrix: Predicted  0.0  1.0  2.0\n",
    "Actual                  \n",
    "0            8    0    0\n",
    "1            0   17    0\n",
    "2            1    0   19\n",
    "\n",
    "Finished after  2000  iterations: theta= [ 0.06630102 -0.83899381  0.05199899  0.4335272  -0.32723222]\n",
    "\n",
    "Finished after  2000  iterations: theta= [ 0.22907638  1.15960626  0.3583813  -1.81893984 -0.80271193]\n",
    "\n",
    "Finished after  2000  iterations: theta= [-0.4180923  -0.89312578 -0.92224671  1.41984154  1.13787278]\n",
    "Fold 4 Accuracy: 0.888888888889\n",
    "Precision, Recall and F_score: (0.9242424242424242, 0.90740740740740744, 0.90350151640474208, None)\n",
    "Confusion Matrix: Predicted  0.0  1.0  2.0\n",
    "Actual                  \n",
    "0           13    0    5\n",
    "1            0   10    0\n",
    "2            0    0   17\n",
    "\n",
    "Finished after  2000  iterations: theta= [ 0.01363075 -0.65617528  0.03333821  0.30711149 -0.27115646]\n",
    "\n",
    "Finished after  2000  iterations: theta= [ 0.23751678  1.12892166  0.34321894 -1.79592148 -0.81199061]\n",
    "\n",
    "Finished after  2000  iterations: theta= [-0.39102498 -1.0515347  -0.90235013  1.53594973  1.0855108 ]\n",
    "Fold 5 Accuracy: 0.777777777778\n",
    "Precision, Recall and F_score: (0.84848484848484851, 0.82456140350877194, 0.78291316526610633, None)\n",
    "Confusion Matrix: Predicted  0.0  1.0  2.0\n",
    "Actual                  \n",
    "0            9    0   10\n",
    "1            0   14    0\n",
    "2            0    0   12\n",
    "\n",
    "Finished after  2000  iterations: theta= [ 0.12901805 -0.72077762 -0.01640336  0.44920296 -0.260281  ]\n",
    "\n",
    "Finished after  2000  iterations: theta= [ 0.22576609  1.16088251  0.35168483 -1.83130534 -0.84405637]\n",
    "\n",
    "Finished after  2000  iterations: theta= [-0.4614179  -0.99790431 -0.88066931  1.44199976  1.10652934]\n",
    "Fold 6 Accuracy: 0.933333333333\n",
    "Precision, Recall and F_score: (0.95238095238095244, 0.93333333333333324, 0.93732193732193725, None)\n",
    "Confusion Matrix: Predicted  0.0  1.0  2.0\n",
    "Actual                  \n",
    "0           12    0    3\n",
    "1            0   12    0\n",
    "2            0    0   18\n",
    "\n",
    "Finished after  2000  iterations: theta= [ 0.10586365 -0.67514884  0.02826976  0.32783773 -0.25471861]\n",
    "\n",
    "Finished after  2000  iterations: theta= [ 0.21964631  1.09587127  0.3510795  -1.80797558 -0.79280022]\n",
    "\n",
    "Finished after  2000  iterations: theta= [-0.44084069 -1.01633824 -0.89230219  1.50916016  1.03571636]\n",
    "Fold 7 Accuracy: 0.866666666667\n",
    "Precision, Recall and F_score: (0.89473684210526316, 0.875, 0.86057692307692302, None)\n",
    "Confusion Matrix: Predicted  0.0  1.0  2.0\n",
    "Actual                  \n",
    "0           10    0    6\n",
    "1            0   16    0\n",
    "2            0    0   13\n",
    "\n",
    "Finished after  2000  iterations: theta= [ 0.10814145 -0.7651334   0.02483855  0.32401414 -0.25328986]\n",
    "\n",
    "Finished after  2000  iterations: theta= [ 0.22012109  1.1592874   0.36416002 -1.81712733 -0.81614572]\n",
    "\n",
    "Finished after  2000  iterations: theta= [-0.4305889  -0.9709106  -0.92936261  1.55106778  1.06421355]\n",
    "Fold 8 Accuracy: 0.644444444444\n",
    "Precision, Recall and F_score: (0.8160919540229884, 0.75757575757575746, 0.68253968253968245, None)\n",
    "Confusion Matrix: Predicted  0.0  1.0  2.0\n",
    "Actual                  \n",
    "0            6    0   16\n",
    "1            0   10    0\n",
    "2            0    0   13\n",
    "\n",
    "Finished after  2000  iterations: theta= [ 0.07936323 -0.55912504 -0.08007645  0.38237127 -0.29358472]\n",
    "\n",
    "Finished after  2000  iterations: theta= [ 0.21751874  1.11278943  0.35247066 -1.77146703 -0.79157893]\n",
    "\n",
    "Finished after  2000  iterations: theta= [-0.41097308 -1.09579174 -0.81509911  1.44421534  1.07878783]\n",
    "Fold 9 Accuracy: 0.866666666667\n",
    "Precision, Recall and F_score: (0.88235294117647056, 0.89473684210526316, 0.86607142857142849, None)\n",
    "Confusion Matrix: Predicted  0.0  1.0  2.0\n",
    "Actual                  \n",
    "0           13    0    6\n",
    "1            0   15    0\n",
    "2            0    0   11\n",
    "\n",
    "Finished after  2000  iterations: theta= [ 0.14526234 -0.6985769   0.00495833  0.39798835 -0.23206944]\n",
    "\n",
    "Finished after  2000  iterations: theta= [ 0.21249024  1.14925333  0.34541407 -1.83277407 -0.80193038]\n",
    "\n",
    "Finished after  2000  iterations: theta= [-0.45363001 -1.02217097 -0.91944209  1.4997036   1.05582103]\n",
    "Fold 10 Accuracy: 0.955555555556\n",
    "Precision, Recall and F_score: (0.96666666666666667, 0.95238095238095244, 0.95681511470985148, None)\n",
    "Confusion Matrix: Predicted  0.0  1.0  2.0\n",
    "Actual                  \n",
    "0           12    0    2\n",
    "1            0   13    0\n",
    "2            0    0   18\n",
    "\n",
    "Average accuracy of a model after 10 fold is 0.88\n",
    "\n",
    "Process finished with exit code 0"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled6.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
